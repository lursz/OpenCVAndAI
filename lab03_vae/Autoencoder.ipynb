{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CANVAS_SIZE: int = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img: np.ndarray) -> None:\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ellipses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ellipses:\n",
    "    def __init__(self) -> None:\n",
    "        self.img_size: int = CANVAS_SIZE\n",
    "\n",
    "    def generate_random_ellipse(self) -> np.ndarray:\n",
    "        img = np.zeros(self.img_size, dtype=np.uint8)\n",
    "        center: tuple[int, int] = (\n",
    "            np.random.randint(int(self.img_size[0] * 0.2), int(self.img_size[0] * 0.8)),\n",
    "            np.random.randint(int(self.img_size[1] * 0.2), int(self.img_size[1] * 0.8))\n",
    "        )\n",
    "        axes: tuple[int, int] = (\n",
    "            np.random.randint(int(self.img_size[0] * 0.2), int(self.img_size[0] * 0.6)),\n",
    "            np.random.randint(int(self.img_size[1] * 0.2), int(self.img_size[1] * 0.6))\n",
    "        )\n",
    "        angle: int = np.random.randint(0, 180)\n",
    "        cv2.ellipse(img, center, axes, angle, 0, 360, 255, -1)\n",
    "        \n",
    "        # show(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def add_random_holes(self, img: np.ndarray, hole_count: int = 30, max_hole_radius: int = 30) -> np.ndarray:\n",
    "        ellipse_points: np.ndarray = np.argwhere(img == 255)\n",
    "\n",
    "        hole_count: int  = np.random.randint(5, hole_count)\n",
    "        for _ in range(hole_count):\n",
    "            if len(ellipse_points) < 50:\n",
    "                break  # otherwise looks like a swiss cheese\n",
    "            \n",
    "            center_y, center_x = ellipse_points[np.random.choice(len(ellipse_points))]\n",
    "            radius = np.random.randint(5, max_hole_radius)\n",
    "            cv2.circle(img, (center_x, center_y), radius, 0, -1)\n",
    "\n",
    "        return img\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ellipses(img1, img2, img3):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    axs[0].imshow(img1, cmap='gray')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(\"Original\")\n",
    "    \n",
    "    axs[1].imshow(img2, cmap='gray')\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title(\"With holes\")\n",
    "    \n",
    "    axs[2].imshow(img3, cmap='gray')\n",
    "    axs[2].axis('off')\n",
    "    axs[2].set_title(\"With holes\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EllipsesDataset(Dataset):\n",
    "    def __init__(self, transform = None):\n",
    "        self.transform = transform\n",
    "        ellipses_generator = Ellipses()\n",
    "        self.ellipses_generator = ellipses_generator\n",
    "        \n",
    "        self.inputs: list = []\n",
    "        self.targets: list = []\n",
    "        for _ in range(100):\n",
    "            img = ellipses_generator.generate_random_ellipse()\n",
    "            self.inputs.append(img)\n",
    "            img = ellipses_generator.add_random_holes(img)\n",
    "            self.targets.append(img)\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_img = self.inputs[idx]\n",
    "        target_img = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            input_img = self.transform(input_img)\n",
    "            target_img = self.transform(target_img)\n",
    "        \n",
    "        return input_img, target_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 3, 3\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # b, 8, 2, 2\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 5, 5\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # b, 8, 15, 15\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # b, 1, 28, 28\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: Autoencoder, criterion: torch.nn.Module, optimizer: Optimizer, dataloader: dict, EPOCHS=5) -> tuple[Autoencoder, list, list, list, list]:\n",
    "    loss_history: list = []\n",
    "    accuracy_history: list = []\n",
    "    val_loss_history: list = []\n",
    "    val_accuracy_history: list = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss: float = 0.0\n",
    "        running_corrects: int = 0\n",
    "        total_pixels: int = 0\n",
    "        \n",
    "        # Training\n",
    "        for inputs, targets in tqdm(dataloader['train']):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # loss\n",
    "            running_loss += loss.item()\n",
    "            # accuracy\n",
    "            preds = (outputs > 0).float()\n",
    "            running_corrects += torch.sum(preds == targets)\n",
    "            total_pixels += targets.numel()\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader['train'])\n",
    "        epoch_accuracy = running_corrects.double() / total_pixels\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_accuracy.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        val_total_pixels = 0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in dataloader['val']:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_targets = val_targets.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                \n",
    "                # loss\n",
    "                val_loss = criterion(val_outputs, val_targets)\n",
    "                val_running_loss += val_loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                val_preds = (val_outputs > 0).float()\n",
    "                val_running_corrects += torch.sum(val_preds == val_targets)\n",
    "                val_total_pixels += val_targets.numel()\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(dataloader['val'])\n",
    "        val_epoch_accuracy = val_running_corrects.double() / val_total_pixels\n",
    "        val_loss_history.append(val_epoch_loss)\n",
    "        val_accuracy_history.append(val_epoch_accuracy.item())\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{EPOCHS} Loss: {epoch_loss:.4f} Accuracy: {epoch_accuracy:.4f} Val Loss: {val_epoch_loss:.4f} Val Accuracy: {val_epoch_accuracy:.4f}')\n",
    "        \n",
    "    history = {\n",
    "        'loss': loss_history,\n",
    "        'accuracy': accuracy_history,\n",
    "        'val_loss': val_loss_history,\n",
    "        'val_accuracy': val_accuracy_history\n",
    "    }\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "datasets: dict[str, Dataset] = {\n",
    "    'train': EllipsesDataset(transform=transform),\n",
    "    'val': EllipsesDataset(transform=transform)\n",
    "}\n",
    "\n",
    "dataloaders: dict[str, DataLoader] = {\n",
    "    'train': DataLoader(datasets['train'], batch_size=32, shuffle=True),\n",
    "    'val': DataLoader(datasets['val'], batch_size=32, shuffle=False)\n",
    "}\n",
    "\n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "_, history = train_model(model, criterion, optimizer, dataloaders, EPOCHS=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
