{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CANVAS_SIZE: int = (256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img: np.ndarray) -> None:\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ellipses(img1, img2, img3):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    axs[0].imshow(img1, cmap='gray')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(\"With holes\")\n",
    "    \n",
    "    axs[1].imshow(img2, cmap='gray')\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title(\"Target\")\n",
    "    \n",
    "    axs[2].imshow(img3, cmap='gray')\n",
    "    axs[2].axis('off')\n",
    "    axs[2].set_title(\"Autoencoder output\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EllipsesDataset(Dataset):\n",
    "    def __init__(self, transform = None):\n",
    "        self.transform = transform\n",
    "            \n",
    "    def __len__(self) -> int:\n",
    "        return 1000\n",
    "    \n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        target_img = self.__generate_random_ellipse()\n",
    "        input_img = self.__add_random_holes(target_img)\n",
    "        \n",
    "        if self.transform:\n",
    "            target_img = self.transform(target_img)\n",
    "            input_img = self.transform(input_img)\n",
    "        \n",
    "        return input_img, target_img\n",
    "    \n",
    "    \n",
    "    def __generate_random_ellipse(self, img_size: tuple[int, int] = (256,256)) -> np.ndarray:\n",
    "        img = np.zeros(img_size, dtype=np.uint8)\n",
    "        center: tuple[int, int] = (\n",
    "            np.random.randint(int(img_size[0] * 0.2), int(img_size[0] * 0.8)),\n",
    "            np.random.randint(int(img_size[1] * 0.2), int(img_size[1] * 0.8))\n",
    "        )\n",
    "        axes: tuple[int, int] = (\n",
    "            np.random.randint(int(img_size[0] * 0.2), int(img_size[0] * 0.6)),\n",
    "            np.random.randint(int(img_size[1] * 0.2), int(img_size[1] * 0.6))\n",
    "        )\n",
    "        angle: int = np.random.randint(0, 180)\n",
    "        cv2.ellipse(img, center, axes, angle, 0, 360, 255, -1)\n",
    "\n",
    "        # show(img)\n",
    "        return img\n",
    "\n",
    "    def __add_random_holes(self, img: np.ndarray, hole_count: int = 30, max_hole_radius: int = 30) -> np.ndarray:\n",
    "        img = img.copy()\n",
    "        ellipse_points: np.ndarray = np.argwhere(img == 255)\n",
    "\n",
    "        hole_count: int  = np.random.randint(5, hole_count)\n",
    "        for _ in range(hole_count):\n",
    "            if len(ellipse_points) < 50:\n",
    "                break  # otherwise looks like a swiss cheese\n",
    "            \n",
    "            center_y, center_x = ellipse_points[np.random.choice(len(ellipse_points))]\n",
    "            radius = np.random.randint(5, max_hole_radius)\n",
    "            cv2.circle(img, (center_x, center_y), radius, 0, -1)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1),  # b, 8, 256, 256\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 16, 3, stride=3, padding=1),  # b, 16, 85, 85\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 42, 42\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 21, 21\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2), # b, 8, 10, 10\n",
    "            nn.Conv2d(8, 8, 3, stride=2, padding=1),  # b, 8, 5, 5\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2)  # b, 8, 3, 3\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 5, stride=2, output_padding=1, padding=1),  # b, 16, 8, 8\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, output_padding=1, padding=1),  # b, 8, 16, 16\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 8, 3, stride=2, output_padding=1, padding=1),  # b, 8, 32, 32\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 4, 3, stride=2, output_padding=1, padding=1),  # b, 1, 64, 64\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(4, 4, 3, stride=2, output_padding=1, padding=1),  # b, 1, 128, 128\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(4, 4, 3, stride=2, output_padding=1, padding=1),  # b, 1, 256, 256\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(4, 4, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(4, 1, 3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: Autoencoder, criterion: torch.nn.Module, optimizer: Optimizer, dataloader: dict, EPOCHS=5) -> tuple[Autoencoder, dict[str, list]]:\n",
    "    loss_history: list = []\n",
    "    accuracy_history: list = []\n",
    "    val_loss_history: list = []\n",
    "    val_accuracy_history: list = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss: float = 0.0\n",
    "        running_corrects: int = 0\n",
    "        total_pixels: int = 0\n",
    "        \n",
    "        # Training\n",
    "        for inputs, targets in tqdm(dataloader['train']):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # loss\n",
    "            running_loss += loss.item()\n",
    "            # accuracy\n",
    "            preds = (outputs > 0.5).float()\n",
    "            running_corrects += torch.sum(preds == targets)\n",
    "            total_pixels += targets.numel()\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader['train'])\n",
    "        epoch_accuracy = running_corrects.double() / total_pixels\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_accuracy.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        val_total_pixels = 0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in dataloader['val']:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_targets = val_targets.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                \n",
    "                # loss\n",
    "                val_loss = criterion(val_outputs, val_targets)\n",
    "                val_running_loss += val_loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                val_preds = (val_outputs > 0.5).float()\n",
    "                val_running_corrects += torch.sum(val_preds == val_targets)\n",
    "                val_total_pixels += val_targets.numel()\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(dataloader['val'])\n",
    "        val_epoch_accuracy = val_running_corrects.double() / val_total_pixels\n",
    "        val_loss_history.append(val_epoch_loss)\n",
    "        val_accuracy_history.append(val_epoch_accuracy.item())\n",
    "        \n",
    "        visualize_ellipses(inputs[0].cpu().detach().numpy().squeeze(), targets[0].cpu().detach().numpy().squeeze(), outputs[0].cpu().detach().numpy().squeeze())\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{EPOCHS} Loss: {epoch_loss:.4f} Accuracy: {epoch_accuracy:.4f} Val Loss: {val_epoch_loss:.4f} Val Accuracy: {val_epoch_accuracy:.4f}')\n",
    "        \n",
    "    history: dict[str, list] = {\n",
    "        'loss': loss_history,\n",
    "        'accuracy': accuracy_history,\n",
    "        'val_loss': val_loss_history,\n",
    "        'val_accuracy': val_accuracy_history\n",
    "    }\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(transforms=[\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "datasets: dict[str, Dataset] = {\n",
    "    'train': EllipsesDataset(transform=transform),\n",
    "    'val': EllipsesDataset(transform=transform)\n",
    "}\n",
    "\n",
    "dataloaders: dict[str, DataLoader] = {\n",
    "    'train': DataLoader(datasets['train'], batch_size=32, shuffle=True),\n",
    "    'val': DataLoader(datasets['val'], batch_size=32, shuffle=False)\n",
    "}\n",
    "\n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train_model(model, criterion, optimizer, dataloaders, EPOCHS=1000)\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), f'results/autoencoder_{datetime.datetime.now().strftime(\"%Y-%m-%d\")}_{history[\"val_accuracy\"][-1]}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path: str = \"results/\" + \"autoencoder_2024-11-11_[0.5848712158203125].pt\"\n",
    "# autoencoder = Autoencoder()\n",
    "# autoencoder.load_state_dict(torch.load(file_path))\n",
    "# autoencoder.to(device)\n",
    "\n",
    "# # Test\n",
    "# autoencoder.eval()\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in dataloaders['val']:\n",
    "#         inputs = inputs.to(device)\n",
    "#         targets = targets.to(device)\n",
    "#         outputs = autoencoder(inputs)\n",
    "#         visualize_ellipses(inputs[0].cpu().detach().numpy().squeeze(), targets[0].cpu().detach().numpy().squeeze(), outputs[0].cpu().detach().numpy().squeeze())\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
